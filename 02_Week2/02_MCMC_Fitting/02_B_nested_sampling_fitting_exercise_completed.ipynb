{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install corner nestle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages\n",
    "import corner,nestle\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some data we created for this example (.dat is a generic filename, it's just a text file)\n",
    "data_filename='https://raw.githubusercontent.com/uofscphysics/STEM_Python_Course/Summer2020/02_Week2/Data/1D_intro_examples.dat'\n",
    "example_data_1D = pandas.read_csv(data_filename,sep=',',header=0)#this file is separated by spaces and its first line contains the names of the columns (header) \n",
    "print(example_data_1D.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the data, with error bars, that we read from file (See Day 2)\n",
    "plt.errorbar(example_data_1D['x'], #x,y,and error are the column names\n",
    "             example_data_1D['y'], \n",
    "             yerr=example_data_1D['error'],#yerr denotes an error in the y-direction for plotting\n",
    "             fmt='.') #fmt is \"format\", saying that I want data marked by \"points\"\n",
    "plt.xlabel('x') #set the x-axis label \n",
    "plt.ylabel('y') #set the y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data were generated with a simple quadratic equation:\n",
    "#ax^2+bx+c. \n",
    "def my_model(x,a,b,c): #We define the model described above\n",
    "    return(a*x**2+b*x+c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An alternative to MCMC is Bayesian Nested Sampling. It is extremely similar but randomly samples the full parameter space instead of sending out individual walkers. MCMC is often better for a many-dimensional model.\n",
    "\n",
    "#### Now create such a likelihood function. Remember that before we used a $\\chi^2$ function to estimate best-fit parameters. In that case, we wanted to minimize the $\\chi^2$ statistic to find best-fit parameters. Now we want a likelihood function maximize instead of minimize, but the principle is the same (calculating how well a given choice of parameters reproduces the observed data given your model). Therefore, you can just use the opposite (negative) of your $\\chi^2$ function as your likelihood function. For now, just leave your function as-is (we can add the negative in later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chisq_likelihood(theta, args):\n",
    "    #This function accepts an argument \"theta\", which is \n",
    "    #a list of model parameters a, b and c. It then calculates\n",
    "    #a chi-square statistic that it returns, which compares\n",
    "    #the observations, errors, and model provided in args.\n",
    "    \n",
    "    x, y, yerr = args #args is a list, so this is the same as x=args[0],y=args[1],yerr=args[2]. x,y, and yerr are numpy arrays\n",
    "    a,b,c = theta #theta is also a list, so it follows the same as args above\n",
    "    model_observations = my_model(x,a,b,c) \n",
    "    inv_sigma2 = 1./yerr**2 #The chi-square statistic contains an inverse-square error, which we calculate here\n",
    "    chisquare = np.sum((y-model_observations)**2*inv_sigma2 )#calculate the chi-square statistic. \n",
    "    return chisquare\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nestle requires a function to maximize with only 1 argument. Create a `loglike` function that accepts one argument (the array of parameter guesses), then returns the $\\chi^2$ result using your $\\chi^2$-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a likelihood function. Let's use a chi-square again.\n",
    "# The package doing the fitting needs a likelihood function\n",
    "# that only accepts the parameters, so we can create this \n",
    "# small wrapper function that calls our original chisq_likelihood\n",
    "# function with the necessary arguments.\n",
    "def loglike(theta):\n",
    "    args=(example_data_1D['x'],\n",
    "          example_data_1D['y'],\n",
    "          example_data_1D['error'])\n",
    "    chisq = chisq_likelihood(theta,args)\n",
    "    #again we need to take the negative of the chi-square because we need a function to maximize\n",
    "    #and the 0.5 comes from the definition of a Gaussian distribution\n",
    "    return -0.5*chisq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to apply bounds as we did for an MCMC. This can be a bit difficult to understand. We are using the same bounds on our parameters as before here `[a=(-20,20),b=(-20,20),c=(-20,20)]`. However, nested sampling always reduces the problem to the range (0,1) for every parameter. Then we always need to provide a function prior_transform, which applies our prior to the model and simultaneously tranforms from this unit space, to our actual parameter space. This essentially means rescaling the parameter from the range (0,1) to the range (-20,20).\n",
    "\n",
    "#### For example, we want a uniform prior on b, meaning bounds from -20 to 20. The algorithm will only try values for b from 0 to 1. Suppose it tries b=0.5. That's halfway between 0 and 1, which means it is halfway between our bounds (-20,20), which is 0, not 0.5. So we need to find a way to change 0.5-->0. Take a second choice of this (guess b=1$\\rightarrow$20 for example), allowing you to solve this system of equations for the necessary parameters. I'll fill this function in for you, then see if you can work out how to write your own for another scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform(parameters):\n",
    "     return np.array([40, 40, 40]) * parameters + np.array([-20, -20, -20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now all we need is to run the nested sampler. We give it the our likelihood function, our prior transformation function, the number of dimensions (model parameters), and the number of \"points\". The number of points is analagous to the walker/sample number for MCMC (i.e. increasing it should result in a slower but more accurate fit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run nested sampling.\n",
    "result_nest = nestle.sample(loglike, prior_transform, ndim=3,npoints=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay let's see what we got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So what did we actually get from this? Let's use another\n",
    "#python package to see the output of the nested sampling.\n",
    "#The dashed line is one way of estimating the best-fit parameters\n",
    "#(the median of the samples). \n",
    "fig = corner.corner(result_nest.samples, #samples is defined above\n",
    "                    weights=result_nest.weights,\n",
    "                    labels=[\"$a$\", \"$b$\",\"$c$\"],#parameter labels\n",
    "                    quantiles=[.5],\n",
    "                    plot_contours=False,plot_density=False,\n",
    "                    plot_datapoints=True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But how close did we get to the true values?\n",
    "#We can take the 50th percentile of the distributions you see above\n",
    "#As our result. Incidentally we could calculate the uncertainty as \n",
    "#well, perhaps as the differences between the 84th and 50th for\n",
    "#the upper uncertainty, and 50th and 16th for the lower uncertainty\n",
    "\n",
    "#axis=0 means we want to calculate percentiles along columns, not rows\n",
    "a_nest, b_nest,c_nest = np.percentile(result_nest.samples, 50,axis=0)\n",
    "a_nest_lower, b_nest_lower,c_nest_lower = np.percentile(result_nest.samples, 16,axis=0)\n",
    "a_nest_upper, b_nest_upper,c_nest_upper = np.percentile(result_nest.samples, 84,axis=0)\n",
    "\n",
    "print('a:',a_nest,'±(',a_nest_upper-a_nest,a_nest-a_nest_lower,')')\n",
    "print('b:',b_nest,'±(',b_nest_upper-b_nest,b_nest-b_nest_lower,')')\n",
    "print('c:',c_nest,'±(',c_nest_upper-c_nest,c_nest-c_nest_lower,')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the result. Show the data (with error bars), with your best-fit line going through. How does it look? You could compare it to your $\\chi^2$ result if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plotting the model over the data\n",
    "plt.errorbar(example_data_1D['x'],\n",
    "             example_data_1D['y'],\n",
    "             yerr=example_data_1D['error'],\n",
    "             fmt='.',\n",
    "             label='Data')\n",
    "\n",
    "plt.plot(example_data_1D['x'],\n",
    "         my_model(example_data_1D['x'],a_nest,b_nest,c_nest),\n",
    "         'r--',#make the line green and dashed\n",
    "         label='Nestle Fit')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not bad! Play with the `npoints`  to see what it does to computation time and accuracy. What extra information do we get from this approach, vs. the simple $\\chi$-square minimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
